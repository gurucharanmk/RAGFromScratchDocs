{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RAG From Scratch","text":""},{"location":"#introduction","title":"Introduction","text":"<p>RAG From Scratch is a modular framework for Retrieval-Augmented Generation (RAG), built entirely from the ground up for educational purposes. This project aims to provide a clear and comprehensive understanding of how RAG functions under the hood, making it an invaluable resource for students, researchers, and anyone interested in learning about document processing, embedding generation, and language model inference.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Built from Scratch: Every component of the framework has been developed from the ground up, offering insights into the foundational elements of RAG.</li> <li>Well-Commented Code: The code is thoroughly commented with examples, making it easy to follow and understand the key concepts and processes.</li> <li>Modular Design: Extend the framework to support additional language models, vector databases, and data sources.</li> <li>Versatile Data Handling: Currently supports PDF files, with the flexibility to easily extend the framework to accommodate additional formats, including PowerPoint (PPT), plain text, and Excel (XLSX) files.</li> <li>Customizable Splitting Strategies: Experiment with different document splitting strategies to observe their effects on retrieval and generation.</li> </ul>"},{"location":"#components","title":"Components","text":""},{"location":"#data-sources","title":"Data Sources","text":"<ul> <li>Base class: <code>DataSource</code></li> <li>Implementations:</li> <li><code>PDFDataSource</code>: Handles PDF document loading</li> </ul>"},{"location":"#language-models","title":"Language Models","text":"<ul> <li>Base class: <code>LanguageModel</code></li> <li>Implementations:</li> <li><code>OllamaModel</code>: Integration with Ollama models</li> </ul>"},{"location":"#vector-databases","title":"Vector Databases","text":"<ul> <li>Base class: <code>VectorDB</code></li> <li>Implementations:</li> <li><code>FAISSVectorDB</code>: FAISS-based vector storage</li> </ul>"},{"location":"#text-processing","title":"Text Processing","text":"<ul> <li><code>RecursiveTextSplitter</code>: Chunks text into manageable segments</li> <li><code>Prompt</code>: Structures interactions with the language model</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<ol> <li>Data ingestion via DataSource</li> <li>Text splitting with RecursiveTextSplitter</li> <li>Embedding generation using LanguageModel</li> <li>Vector storage in VectorDB</li> <li>Query processing through RAG pipeline</li> </ol>"},{"location":"#api-reference","title":"API Reference","text":"<p>For detailed information about the system's components and their usage, check out our API Reference.</p>"},{"location":"#note","title":"Note","text":"<p>This project is not intended for production use. Instead, it serves as a practical platform for understanding the principles and processes involved in Retrieval-Augmented Generation. Dive in to explore and learn!</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"LICENSE/","title":"The MIT License (MIT)","text":"<p>Copyright \u00a9 <code>2024</code> <code>Gurucharan MK</code></p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>pip (Python package installer)</li> <li>Make (for running the Makefile)</li> <li>Poetry (for packaging and dependency management)</li> </ul>"},{"location":"installation/#steps","title":"Steps","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/gurucharanmk/RAGFromScratch\ncd RAGFromScratch\n</code></pre></p> </li> <li> <p>Install dependencies and set up pre-commit hooks: <pre><code>make install\n</code></pre></p> </li> <li> <p>Set up Ollama: <pre><code>make ollama\n</code></pre></p> </li> <li> <p>Run RAG: <pre><code>#main.py\nfrom src.config.model_config import OllamaConfig\nfrom src.config.vector_db_config import FAISSConfig\nfrom src.config.data_source_config import PDFConfig\nfrom src.rag_system import RAGSystem\n\n# Initialize configs\nollama_config = OllamaConfig(llm_model=\"llama2\")\nfaiss_config = FAISSConfig(index_path=\"./data/vector_store/vdb.index\")\npdf_config = PDFConfig(pdf_path=\"./data/source/document.pdf\")\n\n# Create and use RAG system\nrag = RAGSystem(ollama_config, faiss_config, pdf_config)\nrag.index_data()\nresponse = rag.query(\"Your question here\")\n</code></pre></p> <pre><code>python main.py  \n</code></pre> </li> </ol>"},{"location":"installation/#additional-commands","title":"Additional Commands","text":"<ul> <li> <p>Run unit tests: <pre><code>make test\n</code></pre></p> </li> <li> <p>Check code coverage: <pre><code>make coverage\n</code></pre></p> </li> <li> <p>Lint code: <pre><code>make lint\n</code></pre></p> </li> <li> <p>Format code: <pre><code>make format\n</code></pre></p> </li> <li> <p>Clean build artifacts: <pre><code>make clean\n</code></pre></p> </li> <li> <p>Build documentation: <pre><code>make docs\n</code></pre></p> </li> <li> <p>Serve documentation locally: <pre><code>make docs-serve\n</code></pre></p> </li> <li> <p>Update dependencies: <pre><code>make update\n</code></pre></p> </li> </ul>"},{"location":"user-guide/","title":"RAG System Usage Guide","text":""},{"location":"user-guide/#overview","title":"Overview","text":"<p>This document explains how to use the RAGFromScratch system, which combines document retrieval with language model generation to provide accurate, context-aware responses to questions.</p>"},{"location":"user-guide/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/#1-configure-components","title":"1. Configure Components","text":"<pre><code>from src.config.model_config import OllamaConfig\nfrom src.config.vector_db_config import FAISSConfig\nfrom src.config.data_source_config import PDFConfig\n\nollama_config = OllamaConfig(llm_model=\"llama2\")\nfaiss_config = FAISSConfig(index_path=\"./data/vector_store/vdb.index\")\npdf_config = PDFConfig(pdf_path=\"./data/source/document.pdf\")\n</code></pre>"},{"location":"user-guide/#2-initialize-rag-system","title":"2. Initialize RAG System","text":"<pre><code>from src.rag_system import RAGSystem\n\nrag = RAGSystem(ollama_config, faiss_config, pdf_config)\n</code></pre>"},{"location":"user-guide/#3-index-documents","title":"3. Index Documents","text":"<pre><code>rag.index_data()\n</code></pre>"},{"location":"user-guide/#4-query-the-system","title":"4. Query the System","text":"<pre><code>response = rag.query(\"Your question here\")\nprint(response)\n</code></pre>"},{"location":"user-guide/#configuration-options","title":"Configuration Options","text":""},{"location":"user-guide/#data-sources","title":"Data Sources","text":"<ul> <li>PDF files (PDFConfig)</li> <li><code>pdf_path</code>: Path to PDF file</li> </ul>"},{"location":"user-guide/#language-models","title":"Language Models","text":"<ul> <li>Ollama (OllamaConfig)</li> <li><code>llm_model</code>: Model name (e.g., \"llama2\")</li> </ul>"},{"location":"user-guide/#vector-databases","title":"Vector Databases","text":"<ul> <li>FAISS (FAISSConfig)</li> <li><code>index_path</code>: Path to store/load FAISS index</li> </ul>"},{"location":"user-guide/#support-and-resources","title":"Support and Resources","text":"<ul> <li>Ollama Documentation: ollama.ai/docs</li> <li>FAISS Documentation: faiss.ai</li> </ul>"},{"location":"user-guide/#license","title":"License","text":"<p>MIT</p>"},{"location":"api-reference/","title":"<code>API Reference Index</code>","text":""},{"location":"api-reference/#core-components","title":"Core Components","text":"<ul> <li>RAGSystem API</li> <li>Prompt API</li> </ul>"},{"location":"api-reference/#models","title":"Models","text":"<ul> <li>LanguageModel API</li> <li>OllamaModel API</li> </ul>"},{"location":"api-reference/#data-sources","title":"Data Sources","text":"<ul> <li>DataSource API</li> <li>PDFDataSource API</li> </ul>"},{"location":"api-reference/#vector-databases","title":"Vector Databases","text":"<ul> <li>VectorDB API</li> <li>FAISSVectorDB API</li> </ul>"},{"location":"api-reference/#text-processing","title":"Text Processing","text":"<ul> <li>TextSplitter API</li> <li>RecursiveTextSplitter API</li> </ul>"},{"location":"api-reference/#configuration","title":"Configuration","text":"<ul> <li>ModelConfig API</li> <li>DataSourceConfig API</li> <li>VectorDBConfig API</li> </ul>"},{"location":"api-reference/prompt/","title":"<code>Prompt</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Prompt class for constructing prompts.</p> <p>This class provides a structure for creating prompts with optional system and AI messages, and a required human message.</p> <p>Attributes:</p> Name Type Description <code>system_message</code> <code>Optional[str]</code> <p>An optional system message to set the context.</p> <code>ai_message</code> <code>Optional[str]</code> <p>An optional AI message to include in the prompt.</p> <code>human_message</code> <code>str</code> <p>The required human message or query.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; prompt = Prompt(\n...     system_message=\"You are a helpful AI assistant.\",\n...     ai_message=\"I'm ready to help!\",\n...     human_message=\"Your question here\"\n... )\n&gt;&gt;&gt; print(prompt.construct_prompt())\nSystem: You are a helpful AI assistant.\nAI: I'm ready to help!\nHuman: Your question here\n</code></pre> Source code in <code>src/prompt.py</code> <pre><code>class Prompt(BaseModel):\n    \"\"\"\n    Prompt class for constructing prompts.\n\n    This class provides a structure for creating prompts with optional system and AI messages,\n    and a required human message.\n\n    Attributes:\n        system_message (Optional[str]): An optional system message to set the context.\n        ai_message (Optional[str]): An optional AI message to include in the prompt.\n        human_message (str): The required human message or query.\n\n    Examples:\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_message=\"You are a helpful AI assistant.\",\n        ...     ai_message=\"I'm ready to help!\",\n        ...     human_message=\"Your question here\"\n        ... )\n        &gt;&gt;&gt; print(prompt.construct_prompt())\n        System: You are a helpful AI assistant.\n        AI: I'm ready to help!\n        Human: Your question here\n    \"\"\"\n\n    system_message: Optional[str] = Field(None, description=\"System message\")\n    ai_message: Optional[str] = Field(None, description=\"AI message\")\n    human_message: str = Field(..., description=\"Human message\")\n\n    def construct_prompt(self) -&gt; str:\n        \"\"\"\n        Construct the full prompt by combining all provided messages.\n\n        Returns:\n            str: The constructed prompt string.\n\n        Examples:\n            &gt;&gt;&gt; prompt = Prompt(\n            ...     system_message=\"You are a helpful AI assistant.\",\n            ...     human_message=\"What's the weather like today?\"\n            ... )\n            &gt;&gt;&gt; print(prompt.construct_prompt())\n            System: You are a helpful AI assistant.\n            Human: What's the weather like today?\n        \"\"\"\n        prompt_parts: list[str] = []\n        if self.system_message:\n            prompt_parts.append(f\"System: {self.system_message}\")\n        if self.ai_message:\n            prompt_parts.append(f\"AI: {self.ai_message}\")\n        prompt_parts.append(f\"Human: {self.human_message}\")\n        return \"\\n\".join(prompt_parts)\n</code></pre>"},{"location":"api-reference/prompt/#src.prompt.Prompt.construct_prompt","title":"<code>construct_prompt()</code>","text":"<p>Construct the full prompt by combining all provided messages.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The constructed prompt string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; prompt = Prompt(\n...     system_message=\"You are a helpful AI assistant.\",\n...     human_message=\"What's the weather like today?\"\n... )\n&gt;&gt;&gt; print(prompt.construct_prompt())\nSystem: You are a helpful AI assistant.\nHuman: What's the weather like today?\n</code></pre> Source code in <code>src/prompt.py</code> <pre><code>def construct_prompt(self) -&gt; str:\n    \"\"\"\n    Construct the full prompt by combining all provided messages.\n\n    Returns:\n        str: The constructed prompt string.\n\n    Examples:\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_message=\"You are a helpful AI assistant.\",\n        ...     human_message=\"What's the weather like today?\"\n        ... )\n        &gt;&gt;&gt; print(prompt.construct_prompt())\n        System: You are a helpful AI assistant.\n        Human: What's the weather like today?\n    \"\"\"\n    prompt_parts: list[str] = []\n    if self.system_message:\n        prompt_parts.append(f\"System: {self.system_message}\")\n    if self.ai_message:\n        prompt_parts.append(f\"AI: {self.ai_message}\")\n    prompt_parts.append(f\"Human: {self.human_message}\")\n    return \"\\n\".join(prompt_parts)\n</code></pre>"},{"location":"api-reference/rag_system/","title":"<code>Retrieval-Augmented Generation</code>","text":"<p>Main class for the RAG system.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ollama_config = OllamaConfig(model_name=\"llama2\")\n&gt;&gt;&gt; faiss_config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n&gt;&gt;&gt; pdf_config = PDFConfig(pdf_path=\"/path/to/documents.pdf\")\n&gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n&gt;&gt;&gt; rag.index_data()\n&gt;&gt;&gt; response = rag.query(\"Your question here\")\n&gt;&gt;&gt; print(response)\n'RAG response here'\n</code></pre> Source code in <code>src/rag_system.py</code> <pre><code>class RAGSystem:\n    \"\"\"\n    Main class for the RAG system.\n\n    Examples:\n        &gt;&gt;&gt; ollama_config = OllamaConfig(model_name=\"llama2\")\n        &gt;&gt;&gt; faiss_config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n        &gt;&gt;&gt; pdf_config = PDFConfig(pdf_path=\"/path/to/documents.pdf\")\n        &gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n        &gt;&gt;&gt; rag.index_data()\n        &gt;&gt;&gt; response = rag.query(\"Your question here\")\n        &gt;&gt;&gt; print(response)\n        'RAG response here'\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        vector_db_config: VectorDBConfig,\n        data_source_config: DataSourceConfig,\n    ) -&gt; None:\n        \"\"\"\n        Initialize RAG system.\n\n        Args:\n            model_config: Configuration for the language model\n            vector_db_config: Configuration for the vector database\n            data_source_config: Configuration for the data source\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; ollama_config = OllamaConfig(model_name=\"llama2\")\n            &gt;&gt;&gt; faiss_config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n            &gt;&gt;&gt; pdf_config = PDFConfig(pdf_path=\"/path/to/documents.pdf\")\n            &gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n        \"\"\"\n        self.model: LanguageModel = self._initialize_model(model_config)\n        self.vector_db: VectorDB = self._initialize_vector_db(vector_db_config)\n        self.data_source: DataSource = self._initialize_data_source(data_source_config)\n        self.text_splitter: RecursiveTextSplitter = RecursiveTextSplitter()\n\n    def _initialize_model(self, config: ModelConfig) -&gt; LanguageModel:\n        \"\"\"\n        Initialize language model based on configuration.\n\n        Args:\n            config: Model configuration object\n\n        Returns:\n            Initialized language model instance\n\n        Examples:\n            &gt;&gt;&gt; ollama_config = OllamaConfig(model_name=\"llama2\")\n            &gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n            &gt;&gt;&gt; model = rag._initialize_model(ollama_config)\n            &gt;&gt;&gt; isinstance(model, OllamaModel)\n            True\n        \"\"\"\n        if isinstance(config, OllamaConfig):\n            return OllamaModel(config)\n        raise ValueError(\"Unsupported model configuration\")\n\n    def _initialize_vector_db(self, config: VectorDBConfig) -&gt; VectorDB:\n        \"\"\"\n        Initialize vector database based on configuration.\n\n        Args:\n            config: Vector database configuration object\n\n        Returns:\n            Initialized vector database instance\n\n        Examples:\n            &gt;&gt;&gt; faiss_config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n            &gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n            &gt;&gt;&gt; vector_db = rag._initialize_vector_db(faiss_config)\n            &gt;&gt;&gt; isinstance(vector_db, FAISSVectorDB)\n            True\n        \"\"\"\n        if isinstance(config, FAISSConfig):\n            return FAISSVectorDB(config)\n        raise ValueError(\"Unsupported vector database configuration\")\n\n    def _initialize_data_source(self, config: DataSourceConfig) -&gt; DataSource:\n        \"\"\"\n        Initialize data source based on configuration.\n\n        Args:\n            config: Data source configuration object\n\n        Returns:\n            Initialized data source instance\n\n        Examples:\n            &gt;&gt;&gt; pdf_config = PDFConfig(pdf_path=\"/path/to/documents.pdf\")\n            &gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n            &gt;&gt;&gt; data_source = rag._initialize_data_source(pdf_config)\n            &gt;&gt;&gt; isinstance(data_source, PDFDataSource)\n            True\n        \"\"\"\n        if isinstance(config, PDFConfig):\n            return PDFDataSource(config)\n        raise ValueError(\"Unsupported data source configuration\")\n\n    def index_data(self) -&gt; None:\n        \"\"\"\n        Index data from the data source into the vector database.\n\n        Args:\n\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n            &gt;&gt;&gt; rag.index_data()\n            &gt;&gt;&gt; # Check if vector_db has embeddings\n            &gt;&gt;&gt; len(rag.vector_db.get_all_embeddings()) &gt; 0\n            True\n        \"\"\"\n        documents: List[str] = self.data_source.load_data()\n        chunks: List[str] = []\n        for doc in documents:\n            chunks.extend(self.text_splitter.split_text(doc))\n        embeddings: List[List[float]] = [\n            self.model.get_embeddings(chunk) for chunk in chunks\n        ]\n        metadata: List[dict[str, str]] = [{\"text\": chunk} for chunk in chunks]\n        self.vector_db.add_embeddings(embeddings, metadata)\n\n    def query(self, query: str, k: int = 1) -&gt; str:\n        \"\"\"\n        Process a query and return the response.\n\n        Args:\n            query: User question string\n            k: Number of similar documents to retrieve\n\n        Returns:\n            Generated response string\n\n        Examples:\n            &gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n            &gt;&gt;&gt; rag.index_data()\n            &gt;&gt;&gt; response = rag.query(\"Your question here\")\n            &gt;&gt;&gt; isinstance(response, str)\n            True\n            &gt;&gt;&gt; len(response) &gt; 0\n            True\n        \"\"\"\n        query_embedding: List[float] = self.model.get_embeddings(query)\n        similar_docs: List[dict[str, str]] = self.vector_db.search(query_embedding, k)\n        context: str = \"\\n\".join([doc[\"text\"] for doc in similar_docs])\n        prompt: Prompt = Prompt(\n            system_message=\"You are a helpful AI assistant. Use the following context to answer the human's question.\",\n            ai_message=f\"Context: {context}\",\n            human_message=query,\n        )\n        return self.model.generate(prompt.construct_prompt())\n</code></pre>"},{"location":"api-reference/rag_system/#src.rag_system.RAGSystem.__init__","title":"<code>__init__(model_config, vector_db_config, data_source_config)</code>","text":"<p>Initialize RAG system.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>ModelConfig</code> <p>Configuration for the language model</p> required <code>vector_db_config</code> <code>VectorDBConfig</code> <p>Configuration for the vector database</p> required <code>data_source_config</code> <code>DataSourceConfig</code> <p>Configuration for the data source</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ollama_config = OllamaConfig(model_name=\"llama2\")\n&gt;&gt;&gt; faiss_config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n&gt;&gt;&gt; pdf_config = PDFConfig(pdf_path=\"/path/to/documents.pdf\")\n&gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n</code></pre> Source code in <code>src/rag_system.py</code> <pre><code>def __init__(\n    self,\n    model_config: ModelConfig,\n    vector_db_config: VectorDBConfig,\n    data_source_config: DataSourceConfig,\n) -&gt; None:\n    \"\"\"\n    Initialize RAG system.\n\n    Args:\n        model_config: Configuration for the language model\n        vector_db_config: Configuration for the vector database\n        data_source_config: Configuration for the data source\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; ollama_config = OllamaConfig(model_name=\"llama2\")\n        &gt;&gt;&gt; faiss_config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n        &gt;&gt;&gt; pdf_config = PDFConfig(pdf_path=\"/path/to/documents.pdf\")\n        &gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n    \"\"\"\n    self.model: LanguageModel = self._initialize_model(model_config)\n    self.vector_db: VectorDB = self._initialize_vector_db(vector_db_config)\n    self.data_source: DataSource = self._initialize_data_source(data_source_config)\n    self.text_splitter: RecursiveTextSplitter = RecursiveTextSplitter()\n</code></pre>"},{"location":"api-reference/rag_system/#src.rag_system.RAGSystem.index_data","title":"<code>index_data()</code>","text":"<p>Index data from the data source into the vector database.</p> <p>Args:</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n&gt;&gt;&gt; rag.index_data()\n&gt;&gt;&gt; # Check if vector_db has embeddings\n&gt;&gt;&gt; len(rag.vector_db.get_all_embeddings()) &gt; 0\nTrue\n</code></pre> Source code in <code>src/rag_system.py</code> <pre><code>def index_data(self) -&gt; None:\n    \"\"\"\n    Index data from the data source into the vector database.\n\n    Args:\n\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n        &gt;&gt;&gt; rag.index_data()\n        &gt;&gt;&gt; # Check if vector_db has embeddings\n        &gt;&gt;&gt; len(rag.vector_db.get_all_embeddings()) &gt; 0\n        True\n    \"\"\"\n    documents: List[str] = self.data_source.load_data()\n    chunks: List[str] = []\n    for doc in documents:\n        chunks.extend(self.text_splitter.split_text(doc))\n    embeddings: List[List[float]] = [\n        self.model.get_embeddings(chunk) for chunk in chunks\n    ]\n    metadata: List[dict[str, str]] = [{\"text\": chunk} for chunk in chunks]\n    self.vector_db.add_embeddings(embeddings, metadata)\n</code></pre>"},{"location":"api-reference/rag_system/#src.rag_system.RAGSystem.query","title":"<code>query(query, k=1)</code>","text":"<p>Process a query and return the response.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>User question string</p> required <code>k</code> <code>int</code> <p>Number of similar documents to retrieve</p> <code>1</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated response string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n&gt;&gt;&gt; rag.index_data()\n&gt;&gt;&gt; response = rag.query(\"Your question here\")\n&gt;&gt;&gt; isinstance(response, str)\nTrue\n&gt;&gt;&gt; len(response) &gt; 0\nTrue\n</code></pre> Source code in <code>src/rag_system.py</code> <pre><code>def query(self, query: str, k: int = 1) -&gt; str:\n    \"\"\"\n    Process a query and return the response.\n\n    Args:\n        query: User question string\n        k: Number of similar documents to retrieve\n\n    Returns:\n        Generated response string\n\n    Examples:\n        &gt;&gt;&gt; rag = RAGSystem(ollama_config, faiss_config, pdf_config)\n        &gt;&gt;&gt; rag.index_data()\n        &gt;&gt;&gt; response = rag.query(\"Your question here\")\n        &gt;&gt;&gt; isinstance(response, str)\n        True\n        &gt;&gt;&gt; len(response) &gt; 0\n        True\n    \"\"\"\n    query_embedding: List[float] = self.model.get_embeddings(query)\n    similar_docs: List[dict[str, str]] = self.vector_db.search(query_embedding, k)\n    context: str = \"\\n\".join([doc[\"text\"] for doc in similar_docs])\n    prompt: Prompt = Prompt(\n        system_message=\"You are a helpful AI assistant. Use the following context to answer the human's question.\",\n        ai_message=f\"Context: {context}\",\n        human_message=query,\n    )\n    return self.model.generate(prompt.construct_prompt())\n</code></pre>"},{"location":"api-reference/config/data_source_config/","title":"<code>Configurations for Data Sources</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base configuration for data sources.</p> Source code in <code>src/config/data_source_config.py</code> <pre><code>class DataSourceConfig(BaseModel):\n    \"\"\"Base configuration for data sources.\"\"\"\n\n    pass\n</code></pre> <p>               Bases: <code>DataSourceConfig</code></p> <p>Configuration for PDF data source.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = PDFConfig(pdf_path=\"/path/to/document.pdf\")\n&gt;&gt;&gt; print(config.pdf_path)\n'/path/to/document.pdf'\n</code></pre> Source code in <code>src/config/data_source_config.py</code> <pre><code>class PDFConfig(DataSourceConfig):\n    \"\"\"\n    Configuration for PDF data source.\n\n    Examples:\n        &gt;&gt;&gt; config = PDFConfig(pdf_path=\"/path/to/document.pdf\")\n        &gt;&gt;&gt; print(config.pdf_path)\n        '/path/to/document.pdf'\n    \"\"\"\n\n    pdf_path: str = Field(..., description=\"Path to the PDF file\")\n</code></pre>"},{"location":"api-reference/config/model_config/","title":"<code>Configurations for Language Models</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base configuration for language models.</p> Source code in <code>src/config/model_config.py</code> <pre><code>class ModelConfig(BaseModel):\n    \"\"\"Base configuration for language models.\"\"\"\n\n    pass\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> <p>Configuration for Ollama models.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = OllamaConfig(model_name=\"llama2\")\n&gt;&gt;&gt; print(config.model_name)\n'llama2'\n</code></pre> Source code in <code>src/config/model_config.py</code> <pre><code>class OllamaConfig(ModelConfig):\n    \"\"\"\n    Configuration for Ollama models.\n\n    Examples:\n        &gt;&gt;&gt; config = OllamaConfig(model_name=\"llama2\")\n        &gt;&gt;&gt; print(config.model_name)\n        'llama2'\n    \"\"\"\n\n    llm_model: str = Field(..., description=\"Name of the Ollama model\")\n</code></pre>"},{"location":"api-reference/config/vector_db_config/","title":"<code>Configurations for Vector Database</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base configuration for vector databases.</p> Source code in <code>src/config/vector_db_config.py</code> <pre><code>class VectorDBConfig(BaseModel):\n    \"\"\"Base configuration for vector databases.\"\"\"\n\n    pass\n</code></pre> <p>               Bases: <code>VectorDBConfig</code></p> <p>Configuration for FAISS vector database.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n&gt;&gt;&gt; print(config.index_path)\n'/path/to/faiss/index'\n</code></pre> Source code in <code>src/config/vector_db_config.py</code> <pre><code>class FAISSConfig(VectorDBConfig):\n    \"\"\"\n    Configuration for FAISS vector database.\n\n    Examples:\n        &gt;&gt;&gt; config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n        &gt;&gt;&gt; print(config.index_path)\n        '/path/to/faiss/index'\n    \"\"\"\n\n    index_path: str = Field(..., description=\"Path to the FAISS index\")\n</code></pre>"},{"location":"api-reference/data_source/base/","title":"<code>Abstract Base Class for Data Loaders</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data sources.</p> Source code in <code>src/data_source/base.py</code> <pre><code>class DataSource(ABC):\n    \"\"\"Abstract base class for data sources.\"\"\"\n\n    @abstractmethod\n    def load_data(self) -&gt; List[str]:\n        \"\"\"\n        Load data from the source.\n\n        Examples:\n            &gt;&gt;&gt; data_source = PDFDataSource(PDFConfig(pdf_path=\"/path/to/document.pdf\"))\n            &gt;&gt;&gt; documents = data_source.load_data()\n            &gt;&gt;&gt; print(len(documents))\n            5\n        \"\"\"\n        raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"api-reference/data_source/base/#src.data_source.base.DataSource.load_data","title":"<code>load_data()</code>  <code>abstractmethod</code>","text":"<p>Load data from the source.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data_source = PDFDataSource(PDFConfig(pdf_path=\"/path/to/document.pdf\"))\n&gt;&gt;&gt; documents = data_source.load_data()\n&gt;&gt;&gt; print(len(documents))\n5\n</code></pre> Source code in <code>src/data_source/base.py</code> <pre><code>@abstractmethod\ndef load_data(self) -&gt; List[str]:\n    \"\"\"\n    Load data from the source.\n\n    Examples:\n        &gt;&gt;&gt; data_source = PDFDataSource(PDFConfig(pdf_path=\"/path/to/document.pdf\"))\n        &gt;&gt;&gt; documents = data_source.load_data()\n        &gt;&gt;&gt; print(len(documents))\n        5\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"api-reference/data_source/pdf_source/","title":"<code>PDF Data Loader</code>","text":"<p>               Bases: <code>DataSource</code></p> <p>PDF implementation of DataSource.</p> <p>This class provides methods to load data from PDF files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ..config.data_source_config import PDFConfig\n&gt;&gt;&gt; config = PDFConfig(pdf_path=\"/path/to/document.pdf\")\n&gt;&gt;&gt; pdf_source = PDFDataSource(config)\n&gt;&gt;&gt; documents = pdf_source.load_data()\n&gt;&gt;&gt; print(len(documents))\n5\n</code></pre> Source code in <code>src/data_source/pdf_source.py</code> <pre><code>class PDFDataSource(DataSource):\n    \"\"\"\n    PDF implementation of DataSource.\n\n    This class provides methods to load data from PDF files.\n\n    Examples:\n        &gt;&gt;&gt; from ..config.data_source_config import PDFConfig\n        &gt;&gt;&gt; config = PDFConfig(pdf_path=\"/path/to/document.pdf\")\n        &gt;&gt;&gt; pdf_source = PDFDataSource(config)\n        &gt;&gt;&gt; documents = pdf_source.load_data()\n        &gt;&gt;&gt; print(len(documents))\n        5\n    \"\"\"\n\n    def __init__(self, config: PDFConfig) -&gt; None:\n        \"\"\"\n        Initialize PDF data source.\n\n        Args:\n            config (PDFConfig): Configuration object for the PDF data source.\n\n        Examples:\n            &gt;&gt;&gt; from ..config.data_source_config import PDFConfig\n            &gt;&gt;&gt; config = PDFConfig(pdf_path=\"/path/to/document.pdf\")\n            &gt;&gt;&gt; pdf_source = PDFDataSource(config)\n        \"\"\"\n        self.pdf_path: str = config.pdf_path\n\n    def load_data(self) -&gt; List[str]:\n        \"\"\"\n        Load data from PDF file.\n\n        Returns:\n            List[str]: A list of strings, where each string represents the text content of a page.\n\n        Examples:\n            &gt;&gt;&gt; pdf_source = PDFDataSource(PDFConfig(pdf_path=\"/path/to/document.pdf\"))\n            &gt;&gt;&gt; documents = pdf_source.load_data()\n            &gt;&gt;&gt; print(len(documents))\n            5\n            &gt;&gt;&gt; print(documents[0][:50])\n            'This is the content of the first page of the PDF...'\n        \"\"\"\n        with open(self.pdf_path, \"rb\") as file:\n            reader: PyPDF2.PdfReader = PyPDF2.PdfReader(file)\n            return [page.extract_text() for page in reader.pages]\n</code></pre>"},{"location":"api-reference/data_source/pdf_source/#src.data_source.pdf_source.PDFDataSource.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize PDF data source.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PDFConfig</code> <p>Configuration object for the PDF data source.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ..config.data_source_config import PDFConfig\n&gt;&gt;&gt; config = PDFConfig(pdf_path=\"/path/to/document.pdf\")\n&gt;&gt;&gt; pdf_source = PDFDataSource(config)\n</code></pre> Source code in <code>src/data_source/pdf_source.py</code> <pre><code>def __init__(self, config: PDFConfig) -&gt; None:\n    \"\"\"\n    Initialize PDF data source.\n\n    Args:\n        config (PDFConfig): Configuration object for the PDF data source.\n\n    Examples:\n        &gt;&gt;&gt; from ..config.data_source_config import PDFConfig\n        &gt;&gt;&gt; config = PDFConfig(pdf_path=\"/path/to/document.pdf\")\n        &gt;&gt;&gt; pdf_source = PDFDataSource(config)\n    \"\"\"\n    self.pdf_path: str = config.pdf_path\n</code></pre>"},{"location":"api-reference/data_source/pdf_source/#src.data_source.pdf_source.PDFDataSource.load_data","title":"<code>load_data()</code>","text":"<p>Load data from PDF file.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings, where each string represents the text content of a page.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pdf_source = PDFDataSource(PDFConfig(pdf_path=\"/path/to/document.pdf\"))\n&gt;&gt;&gt; documents = pdf_source.load_data()\n&gt;&gt;&gt; print(len(documents))\n5\n&gt;&gt;&gt; print(documents[0][:50])\n'This is the content of the first page of the PDF...'\n</code></pre> Source code in <code>src/data_source/pdf_source.py</code> <pre><code>def load_data(self) -&gt; List[str]:\n    \"\"\"\n    Load data from PDF file.\n\n    Returns:\n        List[str]: A list of strings, where each string represents the text content of a page.\n\n    Examples:\n        &gt;&gt;&gt; pdf_source = PDFDataSource(PDFConfig(pdf_path=\"/path/to/document.pdf\"))\n        &gt;&gt;&gt; documents = pdf_source.load_data()\n        &gt;&gt;&gt; print(len(documents))\n        5\n        &gt;&gt;&gt; print(documents[0][:50])\n        'This is the content of the first page of the PDF...'\n    \"\"\"\n    with open(self.pdf_path, \"rb\") as file:\n        reader: PyPDF2.PdfReader = PyPDF2.PdfReader(file)\n        return [page.extract_text() for page in reader.pages]\n</code></pre>"},{"location":"api-reference/models/base/","title":"<code>Abstract Base Class for Language Models</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for language models.</p> Source code in <code>src/models/base.py</code> <pre><code>class LanguageModel(ABC):\n    \"\"\"Abstract base class for language models.\"\"\"\n\n    @abstractmethod\n    def generate(self, prompt: str) -&gt; str:\n        \"\"\"\n        Generate text based on the given prompt.\n\n        Examples:\n            &gt;&gt;&gt; model.generate(\"Your question here\")\n            'LLM response here.'\n        \"\"\"\n        raise NotImplementedError  # pragma: no cover\n\n    @abstractmethod\n    def get_embeddings(self, text: str) -&gt; List[float]:\n        \"\"\"\n        Get embeddings for the given text.\n\n        Examples:\n            &gt;&gt;&gt; embeddings = model.get_embeddings(\"Hello, world!\")\n            &gt;&gt;&gt; print(len(embeddings))\n            768\n        \"\"\"\n        raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"api-reference/models/base/#src.models.base.LanguageModel.generate","title":"<code>generate(prompt)</code>  <code>abstractmethod</code>","text":"<p>Generate text based on the given prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model.generate(\"Your question here\")\n'LLM response here.'\n</code></pre> Source code in <code>src/models/base.py</code> <pre><code>@abstractmethod\ndef generate(self, prompt: str) -&gt; str:\n    \"\"\"\n    Generate text based on the given prompt.\n\n    Examples:\n        &gt;&gt;&gt; model.generate(\"Your question here\")\n        'LLM response here.'\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"api-reference/models/base/#src.models.base.LanguageModel.get_embeddings","title":"<code>get_embeddings(text)</code>  <code>abstractmethod</code>","text":"<p>Get embeddings for the given text.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; embeddings = model.get_embeddings(\"Hello, world!\")\n&gt;&gt;&gt; print(len(embeddings))\n768\n</code></pre> Source code in <code>src/models/base.py</code> <pre><code>@abstractmethod\ndef get_embeddings(self, text: str) -&gt; List[float]:\n    \"\"\"\n    Get embeddings for the given text.\n\n    Examples:\n        &gt;&gt;&gt; embeddings = model.get_embeddings(\"Hello, world!\")\n        &gt;&gt;&gt; print(len(embeddings))\n        768\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"api-reference/models/ollama_model/","title":"<code>Ollama Language Model</code>","text":"<p>               Bases: <code>LanguageModel</code></p> <p>Ollama implementation of LanguageModel.</p> <p>This class provides methods to generate text and embeddings using Ollama models.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ..config.model_config import OllamaConfig\n&gt;&gt;&gt; config = OllamaConfig(llm_model=\"llama2\")\n&gt;&gt;&gt; model = OllamaModel(config)\n&gt;&gt;&gt; response = model.generate(\"Your question here\")\n&gt;&gt;&gt; print(response)\n'LLM response here.'\n</code></pre> Source code in <code>src/models/ollama_model.py</code> <pre><code>class OllamaModel(LanguageModel):\n    \"\"\"\n    Ollama implementation of LanguageModel.\n\n    This class provides methods to generate text and embeddings using Ollama models.\n\n    Examples:\n        &gt;&gt;&gt; from ..config.model_config import OllamaConfig\n        &gt;&gt;&gt; config = OllamaConfig(llm_model=\"llama2\")\n        &gt;&gt;&gt; model = OllamaModel(config)\n        &gt;&gt;&gt; response = model.generate(\"Your question here\")\n        &gt;&gt;&gt; print(response)\n        'LLM response here.'\n    \"\"\"\n\n    def __init__(self, config: OllamaConfig) -&gt; None:\n        \"\"\"\n        Initialize Ollama model for generation and embedding.\n\n        Args:\n            config (OllamaConfig): Configuration object for the Ollama model.\n\n        Examples:\n            &gt;&gt;&gt; from ..config.model_config import OllamaConfig\n            &gt;&gt;&gt; config = OllamaConfig(llm_model=\"llama3.2\")\n            &gt;&gt;&gt; model = OllamaModel(config)\n        \"\"\"\n        self.model_name: str = config.llm_model\n\n    def generate(self, prompt: str) -&gt; str:\n        \"\"\"\n        Generate text using Ollama model.\n\n        Args:\n            prompt (str): The input prompt for text generation.\n\n        Returns:\n            str: The generated text response.\n\n        Examples:\n            &gt;&gt;&gt; model = OllamaModel(OllamaConfig(llm_model=\"llama2\"))\n            &gt;&gt;&gt; response = model.generate(\"Who is father of AI.\")\n            &gt;&gt;&gt; print(response)\n            'The title \"father of AI\" is often attributed to John McCarthy, an American computer scientist who coined the term \"artificial intelligence\" in 1956....'\n        \"\"\"\n        response: dict[str, str] = ollama.generate(model=self.model_name, prompt=prompt)\n        return response[\"response\"]\n\n    def get_embeddings(self, text: str) -&gt; List[float]:\n        \"\"\"\n        Get embeddings using Ollama model.\n\n        Args:\n            text (str): The input text to generate embeddings for.\n\n        Returns:\n            List[float]: A list of floating-point numbers representing the embedding.\n\n        Examples:\n            &gt;&gt;&gt; model = OllamaModel(OllamaConfig(llm_model=\"llama2\"))\n            &gt;&gt;&gt; embeddings = model.get_embeddings(\"Hello, world!\")\n            &gt;&gt;&gt; print(len(embeddings))\n            4096  # The actual number may vary depending on the model\n            &gt;&gt;&gt; print(embeddings[:5])\n            [0.023, -0.041, 0.017, 0.089, -0.032]  # Example values\n        \"\"\"\n        response: dict[str, List[float]] = ollama.embeddings(\n            model=self.model_name, prompt=text\n        )\n        return response[\"embedding\"]\n</code></pre>"},{"location":"api-reference/models/ollama_model/#src.models.ollama_model.OllamaModel.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize Ollama model for generation and embedding.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OllamaConfig</code> <p>Configuration object for the Ollama model.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ..config.model_config import OllamaConfig\n&gt;&gt;&gt; config = OllamaConfig(llm_model=\"llama3.2\")\n&gt;&gt;&gt; model = OllamaModel(config)\n</code></pre> Source code in <code>src/models/ollama_model.py</code> <pre><code>def __init__(self, config: OllamaConfig) -&gt; None:\n    \"\"\"\n    Initialize Ollama model for generation and embedding.\n\n    Args:\n        config (OllamaConfig): Configuration object for the Ollama model.\n\n    Examples:\n        &gt;&gt;&gt; from ..config.model_config import OllamaConfig\n        &gt;&gt;&gt; config = OllamaConfig(llm_model=\"llama3.2\")\n        &gt;&gt;&gt; model = OllamaModel(config)\n    \"\"\"\n    self.model_name: str = config.llm_model\n</code></pre>"},{"location":"api-reference/models/ollama_model/#src.models.ollama_model.OllamaModel.generate","title":"<code>generate(prompt)</code>","text":"<p>Generate text using Ollama model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated text response.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = OllamaModel(OllamaConfig(llm_model=\"llama2\"))\n&gt;&gt;&gt; response = model.generate(\"Who is father of AI.\")\n&gt;&gt;&gt; print(response)\n'The title \"father of AI\" is often attributed to John McCarthy, an American computer scientist who coined the term \"artificial intelligence\" in 1956....'\n</code></pre> Source code in <code>src/models/ollama_model.py</code> <pre><code>def generate(self, prompt: str) -&gt; str:\n    \"\"\"\n    Generate text using Ollama model.\n\n    Args:\n        prompt (str): The input prompt for text generation.\n\n    Returns:\n        str: The generated text response.\n\n    Examples:\n        &gt;&gt;&gt; model = OllamaModel(OllamaConfig(llm_model=\"llama2\"))\n        &gt;&gt;&gt; response = model.generate(\"Who is father of AI.\")\n        &gt;&gt;&gt; print(response)\n        'The title \"father of AI\" is often attributed to John McCarthy, an American computer scientist who coined the term \"artificial intelligence\" in 1956....'\n    \"\"\"\n    response: dict[str, str] = ollama.generate(model=self.model_name, prompt=prompt)\n    return response[\"response\"]\n</code></pre>"},{"location":"api-reference/models/ollama_model/#src.models.ollama_model.OllamaModel.get_embeddings","title":"<code>get_embeddings(text)</code>","text":"<p>Get embeddings using Ollama model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to generate embeddings for.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: A list of floating-point numbers representing the embedding.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = OllamaModel(OllamaConfig(llm_model=\"llama2\"))\n&gt;&gt;&gt; embeddings = model.get_embeddings(\"Hello, world!\")\n&gt;&gt;&gt; print(len(embeddings))\n4096  # The actual number may vary depending on the model\n&gt;&gt;&gt; print(embeddings[:5])\n[0.023, -0.041, 0.017, 0.089, -0.032]  # Example values\n</code></pre> Source code in <code>src/models/ollama_model.py</code> <pre><code>def get_embeddings(self, text: str) -&gt; List[float]:\n    \"\"\"\n    Get embeddings using Ollama model.\n\n    Args:\n        text (str): The input text to generate embeddings for.\n\n    Returns:\n        List[float]: A list of floating-point numbers representing the embedding.\n\n    Examples:\n        &gt;&gt;&gt; model = OllamaModel(OllamaConfig(llm_model=\"llama2\"))\n        &gt;&gt;&gt; embeddings = model.get_embeddings(\"Hello, world!\")\n        &gt;&gt;&gt; print(len(embeddings))\n        4096  # The actual number may vary depending on the model\n        &gt;&gt;&gt; print(embeddings[:5])\n        [0.023, -0.041, 0.017, 0.089, -0.032]  # Example values\n    \"\"\"\n    response: dict[str, List[float]] = ollama.embeddings(\n        model=self.model_name, prompt=text\n    )\n    return response[\"embedding\"]\n</code></pre>"},{"location":"api-reference/text_splitter/base/","title":"<code>Abstract Base Class for Text Splitters</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text splitters.</p> Source code in <code>src/text_splitter/base.py</code> <pre><code>class TextSplitter(ABC):\n    \"\"\"Abstract base class for text splitters.\"\"\"\n\n    @abstractmethod\n    def split_text(self, text: str) -&gt; List[str]:\n        \"\"\"\n        Split the input text into chunks.\n\n        Examples:\n            &gt;&gt;&gt; splitter = RecursiveTextSplitter(chunk_size=100, chunk_overlap=20)\n            &gt;&gt;&gt; text = \"This is a long piece of text that needs to be split into smaller chunks.\"\n            &gt;&gt;&gt; chunks = splitter.split_text(text)\n            &gt;&gt;&gt; print(len(chunks))\n            2\n        \"\"\"\n        raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"api-reference/text_splitter/base/#src.text_splitter.base.TextSplitter.split_text","title":"<code>split_text(text)</code>  <code>abstractmethod</code>","text":"<p>Split the input text into chunks.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; splitter = RecursiveTextSplitter(chunk_size=100, chunk_overlap=20)\n&gt;&gt;&gt; text = \"This is a long piece of text that needs to be split into smaller chunks.\"\n&gt;&gt;&gt; chunks = splitter.split_text(text)\n&gt;&gt;&gt; print(len(chunks))\n2\n</code></pre> Source code in <code>src/text_splitter/base.py</code> <pre><code>@abstractmethod\ndef split_text(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Split the input text into chunks.\n\n    Examples:\n        &gt;&gt;&gt; splitter = RecursiveTextSplitter(chunk_size=100, chunk_overlap=20)\n        &gt;&gt;&gt; text = \"This is a long piece of text that needs to be split into smaller chunks.\"\n        &gt;&gt;&gt; chunks = splitter.split_text(text)\n        &gt;&gt;&gt; print(len(chunks))\n        2\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"api-reference/text_splitter/recursive_splitter/","title":"<code>Recursive Text Splitter</code>","text":"<p>               Bases: <code>TextSplitter</code></p> <p>RecursiveTextSplitter implementation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; splitter = RecursiveTextSplitter(chunk_size=100, chunk_overlap=20)\n&gt;&gt;&gt; text = \"This is a long piece of text that needs to be split into smaller chunks.\"\n&gt;&gt;&gt; chunks = splitter.split_text(text)\n&gt;&gt;&gt; print(len(chunks))\n2\n</code></pre> Source code in <code>src/text_splitter/recursive_splitter.py</code> <pre><code>class RecursiveTextSplitter(TextSplitter):\n    \"\"\"\n    RecursiveTextSplitter implementation.\n\n    Examples:\n        &gt;&gt;&gt; splitter = RecursiveTextSplitter(chunk_size=100, chunk_overlap=20)\n        &gt;&gt;&gt; text = \"This is a long piece of text that needs to be split into smaller chunks.\"\n        &gt;&gt;&gt; chunks = splitter.split_text(text)\n        &gt;&gt;&gt; print(len(chunks))\n        2\n    \"\"\"\n\n    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200) -&gt; None:\n        \"\"\"\n        Initialize RecursiveTextSplitter.\n\n        Args:\n            chunk_size: Maximum size of each text chunk\n            chunk_overlap: Number of characters to overlap between chunks\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; splitter = RecursiveTextSplitter(chunk_size=500, chunk_overlap=50)\n            &gt;&gt;&gt; splitter.chunk_size\n            500\n            &gt;&gt;&gt; splitter.chunk_overlap\n            50\n        \"\"\"\n        self.chunk_size: int = chunk_size\n        self.chunk_overlap: int = chunk_overlap\n\n    def split_text(self, text: str) -&gt; List[str]:\n        \"\"\"\n        Split the input text into chunks recursively with overlap.\n\n        Args:\n            text: Input text string to split\n\n        Returns:\n            List of text chunks as strings\n\n        Examples:\n            &gt;&gt;&gt; splitter = RecursiveTextSplitter(chunk_size=10, chunk_overlap=2)\n            &gt;&gt;&gt; text = \"This is a test text for splitting.\"\n            &gt;&gt;&gt; chunks = splitter.split_text(text)\n            &gt;&gt;&gt; print(chunks)\n            ['This is a ', 'a test tex', 'text for s', 'splitting.']\n        \"\"\"\n        chunks: List[str] = []\n        start: int = 0\n        while start &lt; len(text):\n            end: int = start + self.chunk_size\n            chunk: str = text[start:end]\n            chunks.append(chunk)\n            start = end - self.chunk_overlap\n        return chunks\n</code></pre>"},{"location":"api-reference/text_splitter/recursive_splitter/#src.text_splitter.recursive_splitter.RecursiveTextSplitter.__init__","title":"<code>__init__(chunk_size=1000, chunk_overlap=200)</code>","text":"<p>Initialize RecursiveTextSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Maximum size of each text chunk</p> <code>1000</code> <code>chunk_overlap</code> <code>int</code> <p>Number of characters to overlap between chunks</p> <code>200</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; splitter = RecursiveTextSplitter(chunk_size=500, chunk_overlap=50)\n&gt;&gt;&gt; splitter.chunk_size\n500\n&gt;&gt;&gt; splitter.chunk_overlap\n50\n</code></pre> Source code in <code>src/text_splitter/recursive_splitter.py</code> <pre><code>def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200) -&gt; None:\n    \"\"\"\n    Initialize RecursiveTextSplitter.\n\n    Args:\n        chunk_size: Maximum size of each text chunk\n        chunk_overlap: Number of characters to overlap between chunks\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; splitter = RecursiveTextSplitter(chunk_size=500, chunk_overlap=50)\n        &gt;&gt;&gt; splitter.chunk_size\n        500\n        &gt;&gt;&gt; splitter.chunk_overlap\n        50\n    \"\"\"\n    self.chunk_size: int = chunk_size\n    self.chunk_overlap: int = chunk_overlap\n</code></pre>"},{"location":"api-reference/text_splitter/recursive_splitter/#src.text_splitter.recursive_splitter.RecursiveTextSplitter.split_text","title":"<code>split_text(text)</code>","text":"<p>Split the input text into chunks recursively with overlap.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text string to split</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of text chunks as strings</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; splitter = RecursiveTextSplitter(chunk_size=10, chunk_overlap=2)\n&gt;&gt;&gt; text = \"This is a test text for splitting.\"\n&gt;&gt;&gt; chunks = splitter.split_text(text)\n&gt;&gt;&gt; print(chunks)\n['This is a ', 'a test tex', 'text for s', 'splitting.']\n</code></pre> Source code in <code>src/text_splitter/recursive_splitter.py</code> <pre><code>def split_text(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Split the input text into chunks recursively with overlap.\n\n    Args:\n        text: Input text string to split\n\n    Returns:\n        List of text chunks as strings\n\n    Examples:\n        &gt;&gt;&gt; splitter = RecursiveTextSplitter(chunk_size=10, chunk_overlap=2)\n        &gt;&gt;&gt; text = \"This is a test text for splitting.\"\n        &gt;&gt;&gt; chunks = splitter.split_text(text)\n        &gt;&gt;&gt; print(chunks)\n        ['This is a ', 'a test tex', 'text for s', 'splitting.']\n    \"\"\"\n    chunks: List[str] = []\n    start: int = 0\n    while start &lt; len(text):\n        end: int = start + self.chunk_size\n        chunk: str = text[start:end]\n        chunks.append(chunk)\n        start = end - self.chunk_overlap\n    return chunks\n</code></pre>"},{"location":"api-reference/vector_db/base/","title":"<code>Abstract Base Class for Vector Database</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for vector databases.</p> Source code in <code>src/vector_db/base.py</code> <pre><code>class VectorDB(ABC):\n    \"\"\"Abstract base class for vector databases.\"\"\"\n\n    @abstractmethod\n    def add_embeddings(\n        self, embeddings: List[List[float]], metadata: List[Dict[str, Any]]\n    ) -&gt; None:\n        \"\"\"\n        Add embeddings to the vector database.\n\n        Examples:\n            &gt;&gt;&gt; embeddings = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n            &gt;&gt;&gt; metadata = [{\"text\": \"Hello\"}, {\"text\": \"World\"}]\n            &gt;&gt;&gt; vector_db.add_embeddings(embeddings, metadata)\n        \"\"\"\n        raise NotImplementedError  # pragma: no cover\n\n    @abstractmethod\n    def search(self, query_embedding: List[float], k: int) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Search for similar embeddings in the vector database.\n\n        Examples:\n            &gt;&gt;&gt; query_embedding = [0.1, 0.2, 0.3]\n            &gt;&gt;&gt; results = vector_db.search(query_embedding, k=2)\n            &gt;&gt;&gt; print(results)\n            [{'distance': 0.1, 'index': 0}, {'distance': 0.2, 'index': 1}]\n        \"\"\"\n        raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"api-reference/vector_db/base/#src.vector_db.base.VectorDB.add_embeddings","title":"<code>add_embeddings(embeddings, metadata)</code>  <code>abstractmethod</code>","text":"<p>Add embeddings to the vector database.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; embeddings = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n&gt;&gt;&gt; metadata = [{\"text\": \"Hello\"}, {\"text\": \"World\"}]\n&gt;&gt;&gt; vector_db.add_embeddings(embeddings, metadata)\n</code></pre> Source code in <code>src/vector_db/base.py</code> <pre><code>@abstractmethod\ndef add_embeddings(\n    self, embeddings: List[List[float]], metadata: List[Dict[str, Any]]\n) -&gt; None:\n    \"\"\"\n    Add embeddings to the vector database.\n\n    Examples:\n        &gt;&gt;&gt; embeddings = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n        &gt;&gt;&gt; metadata = [{\"text\": \"Hello\"}, {\"text\": \"World\"}]\n        &gt;&gt;&gt; vector_db.add_embeddings(embeddings, metadata)\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"api-reference/vector_db/base/#src.vector_db.base.VectorDB.search","title":"<code>search(query_embedding, k)</code>  <code>abstractmethod</code>","text":"<p>Search for similar embeddings in the vector database.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; query_embedding = [0.1, 0.2, 0.3]\n&gt;&gt;&gt; results = vector_db.search(query_embedding, k=2)\n&gt;&gt;&gt; print(results)\n[{'distance': 0.1, 'index': 0}, {'distance': 0.2, 'index': 1}]\n</code></pre> Source code in <code>src/vector_db/base.py</code> <pre><code>@abstractmethod\ndef search(self, query_embedding: List[float], k: int) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Search for similar embeddings in the vector database.\n\n    Examples:\n        &gt;&gt;&gt; query_embedding = [0.1, 0.2, 0.3]\n        &gt;&gt;&gt; results = vector_db.search(query_embedding, k=2)\n        &gt;&gt;&gt; print(results)\n        [{'distance': 0.1, 'index': 0}, {'distance': 0.2, 'index': 1}]\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"api-reference/vector_db/faiss_db/","title":"<code>FAISS Vector Database</code>","text":"<p>               Bases: <code>VectorDB</code></p> <p>FAISS implementation of VectorDB</p> <p>This class provides methods to add embeddings and search for similar embeddings using FAISS.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ..config.vector_db_config import FAISSConfig\n&gt;&gt;&gt; config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n&gt;&gt;&gt; vector_db = FAISSVectorDB(config)\n&gt;&gt;&gt; embeddings = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n&gt;&gt;&gt; metadata = [{\"text\": \"Hello\"}, {\"text\": \"World\"}]\n&gt;&gt;&gt; vector_db.add_embeddings(embeddings, metadata)\n&gt;&gt;&gt; query_embedding = [0.1, 0.2, 0.3]\n&gt;&gt;&gt; results = vector_db.search(query_embedding, k=1)\n&gt;&gt;&gt; print(results)\n[{'distance': 0.0, 'index': 0, 'text': 'Hello'}]\n</code></pre> Source code in <code>src/vector_db/faiss_db.py</code> <pre><code>class FAISSVectorDB(VectorDB):\n    \"\"\"\n    FAISS implementation of VectorDB\n\n    This class provides methods to add embeddings and search for similar embeddings using FAISS.\n\n    Examples:\n        &gt;&gt;&gt; from ..config.vector_db_config import FAISSConfig\n        &gt;&gt;&gt; config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n        &gt;&gt;&gt; vector_db = FAISSVectorDB(config)\n        &gt;&gt;&gt; embeddings = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n        &gt;&gt;&gt; metadata = [{\"text\": \"Hello\"}, {\"text\": \"World\"}]\n        &gt;&gt;&gt; vector_db.add_embeddings(embeddings, metadata)\n        &gt;&gt;&gt; query_embedding = [0.1, 0.2, 0.3]\n        &gt;&gt;&gt; results = vector_db.search(query_embedding, k=1)\n        &gt;&gt;&gt; print(results)\n        [{'distance': 0.0, 'index': 0, 'text': 'Hello'}]\n    \"\"\"\n\n    def __init__(self, config: FAISSConfig) -&gt; None:\n        \"\"\"\n        Initialize FAISS vector database.\n\n        Args:\n            config (FAISSConfig): Configuration object for the FAISS vector database.\n\n        Examples:\n            &gt;&gt;&gt; from ..config.vector_db_config import FAISSConfig\n            &gt;&gt;&gt; config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n            &gt;&gt;&gt; vector_db = FAISSVectorDB(config)\n        \"\"\"\n        self.file_path: str = config.index_path\n        self.index: Optional[faiss.Index] = None\n        self.dimension: Optional[int] = None\n        self.texts: List[str] = []\n\n    '''\n    def _load_or_create_index(self) -&gt; None:\n        \"\"\"\n        Load existing index or prepare for creating a new one.\n\n        Examples:\n            &gt;&gt;&gt; vector_db = FAISSVectorDB(FAISSConfig(index_path=\"/path/to/faiss/index\"))\n            &gt;&gt;&gt; vector_db._load_or_create_index()\n        \"\"\"\n        if os.path.exists(self.file_path):\n            self.index = faiss.read_index(self.file_path)\n            print(f\"Loaded existing index from {self.file_path}\")\n        else:\n            print(\n                f\"Index file not found at {self.file_path}. It will be created when adding embeddings.\"\n            )\n    '''\n\n    def add_embeddings(\n        self, embeddings: List[List[float]], metadata: List[Dict[str, Any]]\n    ) -&gt; None:\n        \"\"\"\n        Add embeddings to FAISS index.\n\n        Args:\n            embeddings (List[List[float]]): List of embedding vectors to add.\n            metadata (List[Dict[str, Any]]): List of metadata dictionaries corresponding to the embeddings.\n\n        Examples:\n            &gt;&gt;&gt; vector_db = FAISSVectorDB(FAISSConfig(index_path=\"/path/to/faiss/index\"))\n            &gt;&gt;&gt; embeddings = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n            &gt;&gt;&gt; metadata = [{\"text\": \"Hello\"}, {\"text\": \"World\"}]\n            &gt;&gt;&gt; vector_db.add_embeddings(embeddings, metadata)\n        \"\"\"\n        embeddings_array: np.ndarray = np.array(embeddings).astype(\"float32\")\n\n        if self.index is None:\n            self.dimension = embeddings_array.shape[1]\n            self.index = faiss.IndexFlatL2(self.dimension)\n            print(f\"Created new index with dimension {self.dimension}\")\n\n        self.index.add(embeddings_array)\n        self.texts.extend([m[\"text\"] for m in metadata])  # Store the text content\n\n        # Save the updated index\n        faiss.write_index(self.index, self.file_path)\n        print(f\"Saved index to {self.file_path}\")\n\n    def search(self, query_embedding: List[float], k: int) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Search for similar embeddings in FAISS index.\n\n        Args:\n            query_embedding (List[float]): The query embedding vector.\n            k (int): The number of nearest neighbors to return.\n\n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries containing search results.\n\n        Examples:\n            &gt;&gt;&gt; vector_db = FAISSVectorDB(FAISSConfig(index_path=\"/path/to/faiss/index\"))\n            &gt;&gt;&gt; query_embedding = [0.1, 0.2, 0.3]\n            &gt;&gt;&gt; results = vector_db.search(query_embedding, k=1)\n            &gt;&gt;&gt; print(results)\n            [{'distance': 0.0, 'index': 0, 'text': 'Hello'}]\n        \"\"\"\n        distances, indices = self.index.search(np.array([query_embedding]), k)\n        return [\n            {\n                \"distance\": float(distances[0][i]),\n                \"index\": int(indices[0][i]),\n                \"text\": self.texts[indices[0][i]],\n            }\n            for i in range(k)\n        ]\n</code></pre>"},{"location":"api-reference/vector_db/faiss_db/#src.vector_db.faiss_db.FAISSVectorDB.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize FAISS vector database.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>FAISSConfig</code> <p>Configuration object for the FAISS vector database.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ..config.vector_db_config import FAISSConfig\n&gt;&gt;&gt; config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n&gt;&gt;&gt; vector_db = FAISSVectorDB(config)\n</code></pre> Source code in <code>src/vector_db/faiss_db.py</code> <pre><code>def __init__(self, config: FAISSConfig) -&gt; None:\n    \"\"\"\n    Initialize FAISS vector database.\n\n    Args:\n        config (FAISSConfig): Configuration object for the FAISS vector database.\n\n    Examples:\n        &gt;&gt;&gt; from ..config.vector_db_config import FAISSConfig\n        &gt;&gt;&gt; config = FAISSConfig(index_path=\"/path/to/faiss/index\")\n        &gt;&gt;&gt; vector_db = FAISSVectorDB(config)\n    \"\"\"\n    self.file_path: str = config.index_path\n    self.index: Optional[faiss.Index] = None\n    self.dimension: Optional[int] = None\n    self.texts: List[str] = []\n</code></pre>"},{"location":"api-reference/vector_db/faiss_db/#src.vector_db.faiss_db.FAISSVectorDB.add_embeddings","title":"<code>add_embeddings(embeddings, metadata)</code>","text":"<p>Add embeddings to FAISS index.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>List[List[float]]</code> <p>List of embedding vectors to add.</p> required <code>metadata</code> <code>List[Dict[str, Any]]</code> <p>List of metadata dictionaries corresponding to the embeddings.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; vector_db = FAISSVectorDB(FAISSConfig(index_path=\"/path/to/faiss/index\"))\n&gt;&gt;&gt; embeddings = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n&gt;&gt;&gt; metadata = [{\"text\": \"Hello\"}, {\"text\": \"World\"}]\n&gt;&gt;&gt; vector_db.add_embeddings(embeddings, metadata)\n</code></pre> Source code in <code>src/vector_db/faiss_db.py</code> <pre><code>def add_embeddings(\n    self, embeddings: List[List[float]], metadata: List[Dict[str, Any]]\n) -&gt; None:\n    \"\"\"\n    Add embeddings to FAISS index.\n\n    Args:\n        embeddings (List[List[float]]): List of embedding vectors to add.\n        metadata (List[Dict[str, Any]]): List of metadata dictionaries corresponding to the embeddings.\n\n    Examples:\n        &gt;&gt;&gt; vector_db = FAISSVectorDB(FAISSConfig(index_path=\"/path/to/faiss/index\"))\n        &gt;&gt;&gt; embeddings = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n        &gt;&gt;&gt; metadata = [{\"text\": \"Hello\"}, {\"text\": \"World\"}]\n        &gt;&gt;&gt; vector_db.add_embeddings(embeddings, metadata)\n    \"\"\"\n    embeddings_array: np.ndarray = np.array(embeddings).astype(\"float32\")\n\n    if self.index is None:\n        self.dimension = embeddings_array.shape[1]\n        self.index = faiss.IndexFlatL2(self.dimension)\n        print(f\"Created new index with dimension {self.dimension}\")\n\n    self.index.add(embeddings_array)\n    self.texts.extend([m[\"text\"] for m in metadata])  # Store the text content\n\n    # Save the updated index\n    faiss.write_index(self.index, self.file_path)\n    print(f\"Saved index to {self.file_path}\")\n</code></pre>"},{"location":"api-reference/vector_db/faiss_db/#src.vector_db.faiss_db.FAISSVectorDB.search","title":"<code>search(query_embedding, k)</code>","text":"<p>Search for similar embeddings in FAISS index.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>List[float]</code> <p>The query embedding vector.</p> required <code>k</code> <code>int</code> <p>The number of nearest neighbors to return.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries containing search results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; vector_db = FAISSVectorDB(FAISSConfig(index_path=\"/path/to/faiss/index\"))\n&gt;&gt;&gt; query_embedding = [0.1, 0.2, 0.3]\n&gt;&gt;&gt; results = vector_db.search(query_embedding, k=1)\n&gt;&gt;&gt; print(results)\n[{'distance': 0.0, 'index': 0, 'text': 'Hello'}]\n</code></pre> Source code in <code>src/vector_db/faiss_db.py</code> <pre><code>def search(self, query_embedding: List[float], k: int) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Search for similar embeddings in FAISS index.\n\n    Args:\n        query_embedding (List[float]): The query embedding vector.\n        k (int): The number of nearest neighbors to return.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries containing search results.\n\n    Examples:\n        &gt;&gt;&gt; vector_db = FAISSVectorDB(FAISSConfig(index_path=\"/path/to/faiss/index\"))\n        &gt;&gt;&gt; query_embedding = [0.1, 0.2, 0.3]\n        &gt;&gt;&gt; results = vector_db.search(query_embedding, k=1)\n        &gt;&gt;&gt; print(results)\n        [{'distance': 0.0, 'index': 0, 'text': 'Hello'}]\n    \"\"\"\n    distances, indices = self.index.search(np.array([query_embedding]), k)\n    return [\n        {\n            \"distance\": float(distances[0][i]),\n            \"index\": int(indices[0][i]),\n            \"text\": self.texts[indices[0][i]],\n        }\n        for i in range(k)\n    ]\n</code></pre>"}]}